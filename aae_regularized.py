# -*- coding: utf-8 -*-
import math
import numpy as np
import chainer, os, collections, six
from chainer import cuda, Variable, optimizers, serializers, function, optimizer
from chainer.utils import type_check
from chainer import functions as F
from chainer import links as L
import aae
from aae import activations

class Conf(aae.Conf):
	def __init__(self):
		super(Conf, self).__init__()
		# number of category
		self.ndim_y = 10

class AAE(aae.AAE):

	def build_discriminator_z(self):
		conf = self.conf

		discriminator_z_attributes = {}
		discriminator_z_units = zip(conf.discriminator_z_hidden_units[:-1], conf.discriminator_z_hidden_units[1:])
		discriminator_z_units += [(conf.discriminator_z_hidden_units[-1], 2)]
		for i, (n_in, n_out) in enumerate(discriminator_z_units):
			discriminator_z_attributes["layer_%i" % i] = L.Linear(n_in, n_out, wscale=conf.wscale)
			if conf.batchnorm_before_activation:
				discriminator_z_attributes["batchnorm_%i" % i] = L.BatchNormalization(n_out)
			else:
				discriminator_z_attributes["batchnorm_%i" % i] = L.BatchNormalization(n_in)
		discriminator_z_attributes["layer_merge_z"] = L.Linear(conf.ndim_z, conf.discriminator_z_hidden_units[0], wscale=conf.wscale)
		discriminator_z_attributes["layer_merge_y"] = L.Linear(conf.ndim_y, conf.discriminator_z_hidden_units[0], wscale=conf.wscale)
		if conf.batchnorm_before_activation:
			discriminator_z_attributes["batchnorm_merge"] = L.BatchNormalization(conf.discriminator_z_hidden_units[0])
		else:
			discriminator_z_attributes["batchnorm_merge"] = L.BatchNormalization(conf.ndim_z)

		discriminator_z = SoftmaxClassifier(**discriminator_z_attributes)
		discriminator_z.n_layers = len(discriminator_z_units)
		discriminator_z.activation_function = conf.discriminator_z_activation_function
		discriminator_z.apply_dropout = conf.discriminator_z_apply_dropout
		discriminator_z.apply_batchnorm = conf.discriminator_z_apply_batchnorm
		discriminator_z.apply_batchnorm_to_input = conf.discriminator_z_apply_batchnorm_to_input
		discriminator_z.batchnorm_before_activation = conf.batchnorm_before_activation

		if conf.gpu_enabled:
			discriminator_z.to_gpu()

		return discriminator_z

	def loss_generator_x_z(self, x, y, noise=None):
		xp = self.xp

		# We fool discriminator into thinking that z_fake comes from the true prior distribution. 
		if isinstance(self.generator_x_z, aae.UniversalApproximatorGenerator):
			z_fake = self.generator_x_z(x, test=False, apply_f=True, noise=noise)
		else:
			z_fake = self.generator_x_z(x, test=False, apply_f=True)
		p_fake = self.discriminator_z(z_fake, y, test=False, softmax=False)

		# 0: Samples from true distribution
		# 1: Samples from generator
		loss = F.softmax_cross_entropy(p_fake, Variable(xp.zeros(p_fake.data.shape[0], dtype=np.int32)))

		return loss

	def train_generator_x_z(self, x, y, noise=None):
		loss = self.loss_generator_x_z(x, y, noise=noise)

		self.zero_grads()
		loss.backward()
		self.update_generator()

		if self.gpu_enabled:
			loss.to_cpu()

		return float(loss.data)

	def loss_discriminator_z(self, x, y, z_true, noise=None):
		xp = self.xp

		# z_true came from true prior distribution
		p_true = self.discriminator_z(z_true, y, test=False, softmax=False)

		# 0: Samples from true distribution
		# 1: Samples from generator
		loss_true = F.softmax_cross_entropy(p_true, Variable(xp.zeros(p_true.data.shape[0], dtype=np.int32)))

		# z_fake was generated by generator
		if isinstance(self.generator_x_z, aae.UniversalApproximatorGenerator):
			z_fake = self.generator_x_z(x, test=False, apply_f=True, noise=noise)
		else:
			z_fake = self.generator_x_z(x, test=False, apply_f=True)
		p_fake = self.discriminator_z(z_fake, y, test=False, softmax=False)
		loss_fake = F.softmax_cross_entropy(p_fake, Variable(xp.ones(p_fake.data.shape[0], dtype=np.int32)))

		return loss_true + loss_fake

	def train_discriminator_z(self, x, y, z_true, noise=None):
		loss = self.loss_discriminator_z(x, y, z_true, noise=noise)

		self.zero_grads()
		loss.backward()
		self.update_discriminator()

		if self.gpu_enabled:
			loss.to_cpu()

		return float(loss.data) / 2.0

class SoftmaxClassifier(aae.SoftmaxClassifier):

	def forward_one_step(self, z, y, test=False):
		f = activations[self.activation_function]

		if self.apply_batchnorm_to_input:
			if self.batchnorm_before_activation:
				merged_input = f(self.batchnorm_merge(self.layer_merge_z(z) + self.layer_merge_y(y), test=test))
			else:
				merged_input = f(self.layer_merge_z(self.batchnorm_merge(z, test=test)) + self.layer_merge_y(y))
		else:
			merged_input = f(self.layer_merge_z(z) + self.layer_merge_y(y))

		chain = [merged_input]
		
		for i in range(self.n_layers):
			u = chain[-1]
			if self.batchnorm_before_activation:
				u = getattr(self, "layer_%i" % i)(u)
			if i == 0:
				if self.apply_batchnorm_to_input:
					u = getattr(self, "batchnorm_%d" % i)(u, test=test)
			elif i == self.n_layers - 1:
				if self.apply_batchnorm and self.batchnorm_before_activation == False:
					u = getattr(self, "batchnorm_%d" % i)(u, test=test)
			else:
				if self.apply_batchnorm:
					u = getattr(self, "batchnorm_%d" % i)(u, test=test)
			if self.batchnorm_before_activation == False:
				u = getattr(self, "layer_%i" % i)(u)
			if i == self.n_layers - 1:
				output = u
			else:
				output = f(u)
				if self.apply_dropout:
					output = F.dropout(output, train=not test)
			chain.append(output)

		return chain[-1]

	def __call__(self, z, y, test=False, softmax=True):
		output = self.forward_one_step(z, y, test=test)
		if softmax:
			return F.softmax(output)
		return output